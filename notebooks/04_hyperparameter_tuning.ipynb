{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c29b16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# notebooks/04_hyperparameter_tuning.ipynb\n",
    "# Run in: VS Code or Colab\n",
    "\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Hyperparameter Tuning for Stock Prediction Models\\n\",\n",
    "    \"Optimize model parameters for best performance\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import sys\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"project_root = Path.cwd().parent\\n\",\n",
    "    \"sys.path.insert(0, str(project_root))\\n\",\n",
    "    \"\\n\",\n",
    "    \"from src.data.data_loader import DataLoader\\n\",\n",
    "    \"from src.models.hyperparameter_tuner import HyperparameterTuner\\n\",\n",
    "    \"from src.models.random_forest import RandomForestModel\\n\",\n",
    "    \"from src.models.xgboost_model import XGBoostModel\\n\",\n",
    "    \"from src.models.base_model import BaseModel\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8-darkgrid')\\n\",\n",
    "    \"%matplotlib inline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load and Prepare Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"data_loader = DataLoader()\\n\",\n",
    "    \"tuner = HyperparameterTuner()\\n\",\n",
    "    \"\\n\",\n",
    "    \"symbol = 'AAPL'\\n\",\n",
    "    \"df = data_loader.load_stock_data(symbol, period='2y')\\n\",\n",
    "    \"\\n\",\n",
    "    \"base_model = RandomForestModel()\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = base_model.split_data(df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Training samples: {len(X_train)}\\\")\\n\",\n",
    "    \"print(f\\\"Testing samples: {len(X_test)}\\\")\\n\",\n",
    "    \"print(f\\\"Features: {X_train.shape[1]}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Random Forest Hyperparameter Tuning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n=== Tuning Random Forest ===\\\")\\n\",\n",
    "    \"print(\\\"This may take several minutes...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"rf_best_params, rf_best_score = tuner.tune_random_forest(X_train, y_train, cv=3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nBest Parameters:\\\")\\n\",\n",
    "    \"for param, value in rf_best_params.items():\\n\",\n",
    "    \"    print(f\\\"  {param}: {value}\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nBest Cross-Validation R² Score: {rf_best_score:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"rf_default = RandomForestModel()\\n\",\n",
    "    \"rf_default.build()\\n\",\n",
    "    \"rf_default.train(X_train, y_train)\\n\",\n",
    "    \"default_metrics = rf_default.evaluate(X_test, y_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"rf_tuned = RandomForestModel(**rf_best_params)\\n\",\n",
    "    \"rf_tuned.build()\\n\",\n",
    "    \"rf_tuned.train(X_train, y_train)\\n\",\n",
    "    \"tuned_metrics = rf_tuned.evaluate(X_test, y_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nDefault Model - R²: {default_metrics['r2_score']:.4f}, RMSE: {default_metrics['rmse']:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"Tuned Model   - R²: {tuned_metrics['r2_score']:.4f}, RMSE: {tuned_metrics['rmse']:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"Improvement   - R²: {(tuned_metrics['r2_score'] - default_metrics['r2_score']):.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. XGBoost Hyperparameter Tuning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n=== Tuning XGBoost ===\\\")\\n\",\n",
    "    \"print(\\\"This may take several minutes...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"xgb_best_params, xgb_best_score = tuner.tune_xgboost(X_train, y_train, cv=3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if xgb_best_params:\\n\",\n",
    "    \"    print(f\\\"\\\\nBest Parameters:\\\")\\n\",\n",
    "    \"    for param, value in xgb_best_params.items():\\n\",\n",
    "    \"        print(f\\\"  {param}: {value}\\\")\\n\",\n",
    "    \"    print(f\\\"\\\\nBest Cross-Validation R² Score: {xgb_best_score:.4f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    xgb_default = XGBoostModel()\\n\",\n",
    "    \"    xgb_default.build()\\n\",\n",
    "    \"    xgb_default.train(X_train, y_train)\\n\",\n",
    "    \"    xgb_default_metrics = xgb_default.evaluate(X_test, y_test)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    xgb_tuned = XGBoostModel(**xgb_best_params)\\n\",\n",
    "    \"    xgb_tuned.build()\\n\",\n",
    "    \"    xgb_tuned.train(X_train, y_train)\\n\",\n",
    "    \"    xgb_tuned_metrics = xgb_tuned.evaluate(X_test, y_test)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nDefault Model - R²: {xgb_default_metrics['r2_score']:.4f}, RMSE: {xgb_default_metrics['rmse']:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Tuned Model   - R²: {xgb_tuned_metrics['r2_score']:.4f}, RMSE: {xgb_tuned_metrics['rmse']:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Improvement   - R²: {(xgb_tuned_metrics['r2_score'] - xgb_default_metrics['r2_score']):.4f}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"XGBoost tuning failed or not available\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. LSTM Lookback Tuning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n=== Tuning LSTM Lookback Period ===\\\")\\n\",\n",
    "    \"print(\\\"This may take several minutes...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"lstm_best_params, lstm_best_score = tuner.tune_lstm(df['Close'].values, y_train, lookback_values=[30, 60, 90])\\n\",\n",
    "    \"\\n\",\n",
    "    \"if lstm_best_params:\\n\",\n",
    "    \"    print(f\\\"\\\\nBest Lookback Period: {lstm_best_params['lookback']}\\\")\\n\",\n",
    "    \"    print(f\\\"Best R² Score: {lstm_best_score:.4f}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"LSTM tuning failed or not available\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Parameter Sensitivity Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"n_estimators_range = [50, 100, 150, 200, 250]\\n\",\n",
    "    \"max_depth_range = [5, 10, 15, 20, 25, 30, None]\\n\",\n",
    "    \"\\n\",\n",
    "    \"n_estimators_scores = []\\n\",\n",
    "    \"for n in n_estimators_range:\\n\",\n",
    "    \"    model = RandomForestModel(n_estimators=n)\\n\",\n",
    "    \"    model.build()\\n\",\n",
    "    \"    model.train(X_train, y_train)\\n\",\n",
    "    \"    metrics = model.evaluate(X_test, y_test)\\n\",\n",
    "    \"    n_estimators_scores.append(metrics['r2_score'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"max_depth_scores = []\\n\",\n",
    "    \"for depth in max_depth_range:\\n\",\n",
    "    \"    model = RandomForestModel(max_depth=depth)\\n\",\n",
    "    \"    model.build()\\n\",\n",
    "    \"    model.train(X_train, y_train)\\n\",\n",
    "    \"    metrics = model.evaluate(X_test, y_test)\\n\",\n",
    "    \"    max_depth_scores.append(metrics['r2_score'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\",\n",
    "    \"\\n\",\n",
    "    \"axes[0].plot(n_estimators_range, n_estimators_scores, marker='o', linewidth=2)\\n\",\n",
    "    \"axes[0].set_title('Effect of n_estimators on Model Performance', fontweight='bold')\\n\",\n",
    "    \"axes[0].set_xlabel('Number of Estimators')\\n\",\n",
    "    \"axes[0].set_ylabel('R² Score')\\n\",\n",
    "    \"axes[0].grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"depth_labels = [str(d) if d is not None else 'None' for d in max_depth_range]\\n\",\n",
    "    \"axes[1].plot(depth_labels, max_depth_scores, marker='o', linewidth=2)\\n\",\n",
    "    \"axes[1].set_title('Effect of max_depth on Model Performance', fontweight='bold')\\n\",\n",
    "    \"axes[1].set_xlabel('Max Depth')\\n\",\n",
    "    \"axes[1].set_ylabel('R² Score')\\n\",\n",
    "    \"axes[1].grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Cross-Validation with Best Parameters\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"cv_results = tuner.cross_validate_with_tuning('random_forest', df, n_splits=5)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if cv_results:\\n\",\n",
    "    \"    print(\\\"\\\\nCross-Validation Results:\\\")\\n\",\n",
    "    \"    print(f\\\"Average R²: {cv_results['avg_r2']:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Average RMSE: {cv_results['avg_rmse']:.4f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    fold_results = cv_results['fold_results']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    r2_scores = [r['metrics']['r2_score'] for r in fold_results]\\n\",\n",
    "    \"    rmse_scores = [r['metrics']['rmse'] for r in fold_results]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    axes[0].bar(range(1, len(r2_scores) + 1), r2_scores)\\n\",\n",
    "    \"    axes[0].axhline(y=cv_results['avg_r2'], color='r', linestyle='--', label='Average')\\n\",\n",
    "    \"    axes[0].set_title('R² Score by Fold', fontweight='bold')\\n\",\n",
    "    \"    axes[0].set_xlabel('Fold')\\n\",\n",
    "    \"    axes[0].set_ylabel('R² Score')\\n\",\n",
    "    \"    axes[0].legend()\\n\",\n",
    "    \"    axes[0].grid(True, alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    axes[1].bar(range(1, len(rmse_scores) + 1), rmse_scores, color='orange')\\n\",\n",
    "    \"    axes[1].axhline(y=cv_results['avg_rmse'], color='r', linestyle='--', label='Average')\\n\",\n",
    "    \"    axes[1].set_title('RMSE by Fold', fontweight='bold')\\n\",\n",
    "    \"    axes[1].set_xlabel('Fold')\\n\",\n",
    "    \"    axes[1].set_ylabel('RMSE')\\n\",\n",
    "    \"    axes[1].legend()\\n\",\n",
    "    \"    axes[1].grid(True, alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Ensemble Weight Optimization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n=== Optimizing Ensemble Weights ===\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"best_weights, best_r2, all_results = tuner.grid_search_ensemble(df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if best_weights:\\n\",\n",
    "    \"    print(f\\\"\\\\nBest Weights:\\\")\\n\",\n",
    "    \"    for model, weight in best_weights.items():\\n\",\n",
    "    \"        print(f\\\"  {model}: {weight:.3f}\\\")\\n\",\n",
    "    \"    print(f\\\"\\\\nBest R² Score: {best_r2:.4f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if all_results:\\n\",\n",
    "    \"        results_df = pd.DataFrame([\\n\",\n",
    "    \"            {'combination': i+1, **result}\\n\",\n",
    "    \"            for i, result in enumerate(all_results)\\n\",\n",
    "    \"        ])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"        plt.bar(results_df['combination'], results_df['r2_score'])\\n\",\n",
    "    \"        plt.axhline(y=best_r2, color='r', linestyle='--', linewidth=2, label='Best')\\n\",\n",
    "    \"        plt.title('Ensemble Performance with Different Weight Combinations', fontweight='bold')\\n\",\n",
    "    \"        plt.xlabel('Weight Combination')\\n\",\n",
    "    \"        plt.ylabel('R² Score')\\n\",\n",
    "    \"        plt.legend()\\n\",\n",
    "    \"        plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Learning Curve Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.model_selection import learning_curve\\n\",\n",
    "    \"\\n\",\n",
    "    \"train_sizes = np.linspace(0.1, 1.0, 10)\\n\",\n",
    "    \"\\n\",\n",
    "    \"rf_model = RandomForestModel(**rf_best_params)\\n\",\n",
    "    \"rf_model.build()\\n\",\n",
    "    \"\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestRegressor\\n\",\n",
    "    \"base_rf = RandomForestRegressor(**rf_best_params, random_state=42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"train_sizes_abs, train_scores, val_scores = learning_curve(\\n\",\n",
    "    \"    base_rf, X_train, y_train,\\n\",\n",
    "    \"    train_sizes=train_sizes,\\n\",\n",
    "    \"    cv=TimeSeriesSplit(n_splits=3),\\n\",\n",
    "    \"    scoring='r2',\\n\",\n",
    "    \"    n_jobs=-1\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"train_mean = np.mean(train_scores, axis=1)\\n\",\n",
    "    \"train_std = np.std(train_scores, axis=1)\\n\",\n",
    "    \"val_mean = np.mean(val_scores, axis=1)\\n\",\n",
    "    \"val_std = np.std(val_scores, axis=1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"plt.plot(train_sizes_abs, train_mean, label='Training Score', marker='o', linewidth=2)\\n\",\n",
    "    \"plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.plot(train_sizes_abs, val_mean, label='Validation Score', marker='o', linewidth=2)\\n\",\n",
    "    \"plt.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.title('Learning Curve - Random Forest (Tuned)', fontweight='bold', fontsize=14)\\n\",\n",
    "    \"plt.xlabel('Training Set Size')\\n\",\n",
    "    \"plt.ylabel('R² Score')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Model Comparison: Before and After Tuning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"comparison_data = [\\n\",\n",
    "    \"    {'Model': 'Random Forest (Default)', **default_metrics},\\n\",\n",
    "    \"    {'Model': 'Random Forest (Tuned)', **tuned_metrics}\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"if xgb_best_params:\\n\",\n",
    "    \"    comparison_data.extend([\\n\",\n",
    "    \"        {'Model': 'XGBoost (Default)', **xgb_default_metrics},\\n\",\n",
    "    \"        {'Model': 'XGBoost (Tuned)', **xgb_tuned_metrics}\\n\",\n",
    "    \"    ])\\n\",\n",
    "    \"\\n\",\n",
    "    \"comparison_df = pd.DataFrame(comparison_data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nModel Performance Comparison:\\\")\\n\",\n",
    "    \"print(comparison_df[['Model', 'r2_score', 'rmse', 'mae', 'mape']])\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(14, 10))\\n\",\n",
    "    \"\\n\",\n",
    "    \"comparison_df.plot(x='Model', y='r2_score', kind='bar', ax=axes[0, 0], legend=False, color='steelblue')\\n\",\n",
    "    \"axes[0, 0].set_title('R² Score Comparison', fontweight='bold')\\n\",\n",
    "    \"axes[0, 0].set_ylabel('R² Score')\\n\",\n",
    "    \"axes[0, 0].tick_params(axis='x', rotation=45)\\n\",\n",
    "    \"axes[0, 0].grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"comparison_df.plot(x='Model', y='rmse', kind='bar', ax=axes[0, 1], legend=False, color='coral')\\n\",\n",
    "    \"axes[0, 1].set_title('RMSE Comparison', fontweight='bold')\\n\",\n",
    "    \"axes[0, 1].set_ylabel('RMSE')\\n\",\n",
    "    \"axes[0, 1].tick_params(axis='x', rotation=45)\\n\",\n",
    "    \"axes[0, 1].grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"comparison_df.plot(x='Model', y='mae', kind='bar', ax=axes[1, 0], legend=False, color='lightgreen')\\n\",\n",
    "    \"axes[1, 0].set_title('MAE Comparison', fontweight='bold')\\n\",\n",
    "    \"axes[1, 0].set_ylabel('MAE')\\n\",\n",
    "    \"axes[1, 0].tick_params(axis='x', rotation=45)\\n\",\n",
    "    \"axes[1, 0].grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"comparison_df.plot(x='Model', y='mape', kind='bar', ax=axes[1, 1], legend=False, color='gold')\\n\",\n",
    "    \"axes[1, 1].set_title('MAPE Comparison', fontweight='bold')\\n\",\n",
    "    \"axes[1, 1].set_ylabel('MAPE (%)')\\n\",\n",
    "    \"axes[1, 1].tick_params(axis='x', rotation=45)\\n\",\n",
    "    \"axes[1, 1].grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 10. Summary and Recommendations\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*70)\\n\",\n",
    "    \"print(\\\"HYPERPARAMETER TUNING SUMMARY\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*70)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n1. Random Forest Optimization:\\\")\\n\",\n",
    "    \"print(f\\\"   Best Parameters: {rf_best_params}\\\")\\n\",\n",
    "    \"print(f\\\"   Performance Improvement: {(tuned_metrics['r2_score'] - default_metrics['r2_score'])*100:.2f}%\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if xgb_best_params:\\n\",\n",
    "    \"    print(\\\"\\\\n2. XGBoost Optimization:\\\")\\n\",\n",
    "    \"    print(f\\\"   Best Parameters: {xgb_best_params}\\\")\\n\",\n",
    "    \"    print(f\\\"   Performance Improvement: {(xgb_tuned_metrics['r2_score'] - xgb_default_metrics['r2_score'])*100:.2f}%\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if best_weights:\\n\",\n",
    "    \"    print(\\\"\\\\n3. Ensemble Weight Optimization:\\\")\\n\",\n",
    "    \"    print(f\\\"   Optimal Weights: {best_weights}\\\")\\n\",\n",
    "    \"    print(f\\\"   Best R² Score: {best_r2:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*70)\\n\",\n",
    "    \"print(\\\"RECOMMENDATIONS:\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*70)\\n\",\n",
    "    \"print(\\\"1. Use tuned hyperparameters for production models\\\")\\n\",\n",
    "    \"print(\\\"2. Re-tune periodically as new data becomes available\\\")\\n\",\n",
    "    \"print(\\\"3. Monitor performance on out-of-sample data\\\")\\n\",\n",
    "    \"print(\\\"4. Consider ensemble methods for optimal results\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*70)\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
